//#[macro_use] extern crate lalrpop_util;

lalrpop_mod!(pub lexer);

//mod word;

use std::cell::RefCell;
use crate::word::Word;
use crate::word::Word::*; // Word::Empty -> Empty
/*
fn main() {
    println!("Hello, world!");
//    get_tokens();
    parse();
    println!("(main) OK.");
}
*/
thread_local!{
    static VBUF: RefCell<Vec<Word>> = RefCell::new(Vec::new());
    static LASTTK: RefCell<Word> = RefCell::new(Empty);
}

pub fn parse() {
    parse_L();
    if nexttoken2() != EOF {
        panic!("(parse) Syntax Error!");
    }
}

fn parse_L() {
    match nexttoken2() {
        EOF => {
            // do nothing
        },
        Print => {
            parse_C();
            parse_L();
        },
        Def => {
            parse_C();
            parse_L();
        },
        _ => {
            panic!("(parse_L) Syntax Error!");
        }
    }
}

fn parse_C() {
    match nexttoken2() {
        Print => {
            eat_token();
            parse_E();
            match nexttoken2() {
                Semic => { eat_token(); },
                _ => { panic!("(parse_C Print) Syntax Error!"); }
            };
        },
        Def => {
            eat_token();
            match nexttoken2() {
                Id => {
                    eat_token();
                    parse_E();
                    match nexttoken2() {
                        Semic => { eat_token(); },
                        _ => { panic!("(parse_C, Def) Syntax Error!"); }
                    }
                },
                _ => { panic!("(parse_C, Def) Syntax Error!"); }
            };
        },
        _ => {
            panic!("(parse_C) Syntax Error!");
        }
    }
}

fn parse_E() {
    match nexttoken2() {
        Id => {
            eat_token();
        },
        Num => {
            eat_token();
        },
        Add => {
            eat_token();
            parse_E();
            parse_E();
        },
        Sub => {
            eat_token();
            parse_E();
            parse_E();
        },
        _ => {
            panic!("(parse_E) Syntax Error!");
        }
    }
}

fn get_tokens() {
//    let mut token_buf: Vec<Word> = Vec::new();
    loop {
//        let tk = yylex(&mut token_buf);
        let tk = nexttoken();
        println!("tk={:#?}", tk);
        if tk == EOF { // EOF
            break;
        }
    }
}

fn eat_token() {
    LASTTK.with(|last_tk| {
        *last_tk.borrow_mut() = Empty;
    });
}

fn nexttoken2() -> Word {
    let mut w = EOF;
    LASTTK.with(|last_tk| {
        w = *last_tk.borrow();
        if w == Empty {
            w = nexttoken();
        }
        *last_tk.borrow_mut() = w;
    });
    println!("(nexttoken2) return {:?}", w);
    w
}

fn nexttoken() -> Word {
    let mut w = EOF;
    VBUF.with(|vbuf| {
        w = yylex(&mut *vbuf.borrow_mut());
    });
    w
}

// tokens: buffer that should be global
// ret: a token of type Word
fn yylex(tokens: &mut Vec<Word>) -> Word {
    // if TOKENS is empty:
    //   reat a line
    //   extract tokens to make TOKENS (by using an LALRPOP Parser)
    // pop a token from TOKENS
    // return one

    if tokens.len() == 0 {
        loop {
            let mut in_str: String = String::new();
            let len = std::io::stdin().read_line(&mut in_str)
                .expect("Failed to read line");
            if in_str.len() == 0 { // len=0 -> EOF
                return EOF;
            }
            in_str = in_str.trim().to_string(); // remove whitespaces
            if in_str.len() >= 1 {
                *tokens = lexer::TListParser::new().parse(&in_str).unwrap();
                break;
            }
            // TOKENS was empty but can not be prepared -> repeat
        }
    }
    tokens.pop().unwrap()
}